{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"brain-lab A Science Institute Workflow for Brain Lab Description TODO: finish description. Welcome to the Science Institute Brain Lab pipeline! This service is designed for a user to upload their raw [ modality type ] data acquired with [ description ], which will then be automatically processed with [ \u2026 ]. The service also provides online Jupyter notebooks to visualize the results. This workflow uses components from open-source packages, including the DataJoint Elements: element and url element and url Please follow the steps listed below to begin working with the platform. Using the SciOps Services Account Setup Create a free account at accounts.datajoint.io Create a free GitHub account at github.com Attention Please email us at support@datajoint.com after you create these accounts so we can ensure your service is configured properly. DataJoint LabBook Insert your experimental session metadata. Log in to DataJoint LabBook https://labbook.datajoint.io/ Host: tutorial-db.datajoint.io Username: <datajoint.io account username> Password: <datajoint.io account password> DataJoint LabBook displays data from your database. Left Column Middle Column Right Column Schemas Tables within a schema Entries within a table Enter subject information In the left column, navigate to the subject schema In the middle column, navigate to the Subject table In the right column, Insert a new subject Enter session information In the left column, navigate to the session schema In the middle column, navigate to the Session table In the right column, Insert a new experimental session for the subject Enter session directory information In the left column, navigate to the session schema In the middle column, navigate to the SessionDirectory table In the right column, Insert a new entry to identify where the data is located (relative to the inbox directory) DataJoint SciViz \u2026 DataJoint Axon (Data Upload) \u2026 DataJoint CodeBook (JupyterHub) \u2026 Thank you for using the Science Institute: Brain Lab cloud-based platform. Installation Note The following is intended for developers and is not required for users of the SciOps services. 1. Clone the repository First, clone a local copy of the project repository and change to the location of that directory: git clone https://github.com/iamamutt/science-institute_brain-lab.git cd \"brain-lab\" 2. Create a new python environment We recommend creating an isolated virtual environment to avoid any conflicts with your existing Python packages or interpreter. You can create a virtual environment by first installing conda / mamba : Create an environment using the packages listed in environment.yml : mamba env create -f environment.yml --force Activate the new environment: conda activate brain_lab 3. Install the package brain-lab After the new virtual environment has been created and activated, install this python package using pip>=62.0 ( pip is already in the list of requirements from the environment.yml file). To avoid installing other additional packages, use the following command (see contrib for extra installing packages): pip install . If you need to uninstall the package, do so with pip : pip uninstall brain-lab Additional setup for local development and testing See the Development setup documentation for information on how to install additional packages and tools for local development and testing environments. Project Organization \u251c\u2500\u2500 .github <- GitHub workflows, templates, and actions. \u251c\u2500\u2500 configs <- Store project/build/analysis configuration files here. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling/plots. \u2502 \u2514\u2500\u2500 raw <- Any original, immutable data files/dumps. \u251c\u2500\u2500 docker <- Docker image content, e.g., Dockerfile, docker-compose.yml \u251c\u2500\u2500 docs <- Directory for MkDocs documentation for gh-pages. \u251c\u2500\u2500 figures <- Generated plots and figures for sharing, reports or documentation. \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number for \u2502 ordering, the creator's initials, and a description. \u2502 For example, '1.0-fw-initial-data-exploration'. \u251c\u2500\u2500 scripts <- Analysis examples or production scripts which rely on \u2502 importing the actual Python package, e.g. running queries. \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 brain_lab <- Actual Python package where the main functionality goes. \u2502 \u2514\u2500\u2500 pipeline <- Main schemas and tables used to run the datajoint pipeline. \u2502 \u2514\u2500\u2500 populate <- Code to run `populate` or to ingest data into the database. \u2502 \u2514\u2500\u2500 support <- Tables to add functionality to the main pipeline modules. \u2502 \u2514\u2500\u2500 tables <- Primary end-user tables defined for easy-of-use. \u2502 \u2514\u2500\u2500 utils <- Package utilities. \u2502 \u2514\u2500\u2500 __init__.py <- Root-level package init file. \u2502 \u2514\u2500\u2500 __main__.py <- Main package script. \u2502 \u2514\u2500\u2500 version.py <- Should only contain the current package version number. \u251c\u2500\u2500 tests <- Unit and integration tests which can be run with `pytest` or `nox`. \u251c\u2500\u2500 .cookiecutter.json <- Options specified during template generation. \u251c\u2500\u2500 .gitignore <- Files and folders to ignore for git. \u251c\u2500\u2500 .pre-commit-config.yaml <- Configuration of pre-commit git hooks. \u251c\u2500\u2500 CHANGELOG.md <- Changelog to keep track of new features and fixes. \u251c\u2500\u2500 CONTRIBUTING.md <- Documentation on how to contribute to the project. \u251c\u2500\u2500 *.code-workspace <- Visual Studio Code workspace file. \u251c\u2500\u2500 dj_local_conf.json <- DataJoint configuration file. \u251c\u2500\u2500 environment.yml <- The conda environment file for new virtual environments. \u251c\u2500\u2500 LICENSE <- Open source license. \u251c\u2500\u2500 mkdocs.yml <- Configuration for building the documentation with MkDocs. \u251c\u2500\u2500 noxfile.py <- `nox` automation file for continuous integration steps. \u251c\u2500\u2500 pyproject.toml <- Build system configuration for the project. \u2514\u2500\u2500 README.md <- The top-level \"read me\" for the repository.","title":"Introduction"},{"location":"#brain-lab","text":"A Science Institute Workflow for Brain Lab","title":"brain-lab"},{"location":"#description","text":"TODO: finish description. Welcome to the Science Institute Brain Lab pipeline! This service is designed for a user to upload their raw [ modality type ] data acquired with [ description ], which will then be automatically processed with [ \u2026 ]. The service also provides online Jupyter notebooks to visualize the results. This workflow uses components from open-source packages, including the DataJoint Elements: element and url element and url Please follow the steps listed below to begin working with the platform.","title":"Description"},{"location":"#using-the-sciops-services","text":"","title":"Using the SciOps Services"},{"location":"#account-setup","text":"Create a free account at accounts.datajoint.io Create a free GitHub account at github.com Attention Please email us at support@datajoint.com after you create these accounts so we can ensure your service is configured properly.","title":"Account Setup"},{"location":"#datajoint-labbook","text":"Insert your experimental session metadata. Log in to DataJoint LabBook https://labbook.datajoint.io/ Host: tutorial-db.datajoint.io Username: <datajoint.io account username> Password: <datajoint.io account password> DataJoint LabBook displays data from your database. Left Column Middle Column Right Column Schemas Tables within a schema Entries within a table Enter subject information In the left column, navigate to the subject schema In the middle column, navigate to the Subject table In the right column, Insert a new subject Enter session information In the left column, navigate to the session schema In the middle column, navigate to the Session table In the right column, Insert a new experimental session for the subject Enter session directory information In the left column, navigate to the session schema In the middle column, navigate to the SessionDirectory table In the right column, Insert a new entry to identify where the data is located (relative to the inbox directory)","title":"DataJoint LabBook"},{"location":"#datajoint-sciviz","text":"\u2026","title":"DataJoint SciViz"},{"location":"#datajoint-axon-data-upload","text":"\u2026","title":"DataJoint Axon (Data Upload)"},{"location":"#datajoint-codebook-jupyterhub","text":"\u2026 Thank you for using the Science Institute: Brain Lab cloud-based platform.","title":"DataJoint CodeBook (JupyterHub)"},{"location":"#installation","text":"Note The following is intended for developers and is not required for users of the SciOps services.","title":"Installation"},{"location":"#1-clone-the-repository","text":"First, clone a local copy of the project repository and change to the location of that directory: git clone https://github.com/iamamutt/science-institute_brain-lab.git cd \"brain-lab\"","title":"1. Clone the repository"},{"location":"#2-create-a-new-python-environment","text":"We recommend creating an isolated virtual environment to avoid any conflicts with your existing Python packages or interpreter. You can create a virtual environment by first installing conda / mamba : Create an environment using the packages listed in environment.yml : mamba env create -f environment.yml --force Activate the new environment: conda activate brain_lab","title":"2. Create a new python environment"},{"location":"#3-install-the-package-brain-lab","text":"After the new virtual environment has been created and activated, install this python package using pip>=62.0 ( pip is already in the list of requirements from the environment.yml file). To avoid installing other additional packages, use the following command (see contrib for extra installing packages): pip install . If you need to uninstall the package, do so with pip : pip uninstall brain-lab","title":"3. Install the package brain-lab"},{"location":"#additional-setup-for-local-development-and-testing","text":"See the Development setup documentation for information on how to install additional packages and tools for local development and testing environments.","title":"Additional setup for local development and testing"},{"location":"#project-organization","text":"\u251c\u2500\u2500 .github <- GitHub workflows, templates, and actions. \u251c\u2500\u2500 configs <- Store project/build/analysis configuration files here. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling/plots. \u2502 \u2514\u2500\u2500 raw <- Any original, immutable data files/dumps. \u251c\u2500\u2500 docker <- Docker image content, e.g., Dockerfile, docker-compose.yml \u251c\u2500\u2500 docs <- Directory for MkDocs documentation for gh-pages. \u251c\u2500\u2500 figures <- Generated plots and figures for sharing, reports or documentation. \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number for \u2502 ordering, the creator's initials, and a description. \u2502 For example, '1.0-fw-initial-data-exploration'. \u251c\u2500\u2500 scripts <- Analysis examples or production scripts which rely on \u2502 importing the actual Python package, e.g. running queries. \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 brain_lab <- Actual Python package where the main functionality goes. \u2502 \u2514\u2500\u2500 pipeline <- Main schemas and tables used to run the datajoint pipeline. \u2502 \u2514\u2500\u2500 populate <- Code to run `populate` or to ingest data into the database. \u2502 \u2514\u2500\u2500 support <- Tables to add functionality to the main pipeline modules. \u2502 \u2514\u2500\u2500 tables <- Primary end-user tables defined for easy-of-use. \u2502 \u2514\u2500\u2500 utils <- Package utilities. \u2502 \u2514\u2500\u2500 __init__.py <- Root-level package init file. \u2502 \u2514\u2500\u2500 __main__.py <- Main package script. \u2502 \u2514\u2500\u2500 version.py <- Should only contain the current package version number. \u251c\u2500\u2500 tests <- Unit and integration tests which can be run with `pytest` or `nox`. \u251c\u2500\u2500 .cookiecutter.json <- Options specified during template generation. \u251c\u2500\u2500 .gitignore <- Files and folders to ignore for git. \u251c\u2500\u2500 .pre-commit-config.yaml <- Configuration of pre-commit git hooks. \u251c\u2500\u2500 CHANGELOG.md <- Changelog to keep track of new features and fixes. \u251c\u2500\u2500 CONTRIBUTING.md <- Documentation on how to contribute to the project. \u251c\u2500\u2500 *.code-workspace <- Visual Studio Code workspace file. \u251c\u2500\u2500 dj_local_conf.json <- DataJoint configuration file. \u251c\u2500\u2500 environment.yml <- The conda environment file for new virtual environments. \u251c\u2500\u2500 LICENSE <- Open source license. \u251c\u2500\u2500 mkdocs.yml <- Configuration for building the documentation with MkDocs. \u251c\u2500\u2500 noxfile.py <- `nox` automation file for continuous integration steps. \u251c\u2500\u2500 pyproject.toml <- Build system configuration for the project. \u2514\u2500\u2500 README.md <- The top-level \"read me\" for the repository.","title":"Project Organization"},{"location":"CHANGELOG/","text":"Changelog","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CONTRIBUTING/","text":"Contributing and Developer Guide Welcome to the brain-lab contributor\u2019s guide. This document focuses on getting any potential contributor familiarized with the DataJoint workflow development process, but other general kinds of contributions are also appreciated. See the DataJoint community contributions page for information. If you are new to using git or have never collaborated on a project previously, please have a look at contribution-guide.org . Other resources are also listed in the excellent guide created by FreeCodeCamp . Python Code of Conduct Please notice that all users and contributors are expected to be open, considerate, reasonable, and respectful . When in doubt, Python Software Foundation\u2019s Code of Conduct is a good reference in terms of behavior guidelines. Issue Reports If you experience bugs or general issues with brain-lab , please have a look at the issue tracker . If you don\u2019t see anything useful there, please feel free to fill out a new issue report. New issue reports should include information about your programming environment (e.g., operating system, Python version) and steps to reproduce the problem. Please also try to simplify the reproduction steps to a very minimal example that still illustrates the problem you\u2019re facing. By removing other factors, you help us to identify the root cause of the issue. Closed Issues Please don\u2019t forget to include the closed issues in your search. Sometimes a solution was already reported, and the problem is considered solved . Documentation Improvements You can help improve the brain-lab docs by making them more readable and coherent, or by adding missing information and correcting mistakes. Quick Documentation Changes Please notice that the GitHub web interface provides a quick way of propose changes in brain-lab \u2018s files. While this mechanism can be tricky for normal code contributions, it works perfectly fine for contributing to the docs, and can be quite handy. brain-lab documentation uses MkDocs as its main documentation compiler. This means that the docs are kept in the same repository as the project code, and that any documentation update is done in the same way as a code contribution. All documentation is written in Markdown , specifically the python-markdown implementation with additional extensions to use features from reStructuredText. Google Style Docstrings Most documentation is attached directly to python objects and written in the Google style to faciliate markdown parsing. See Google style docstrings for examples. When working on documentation changes in your local machine, you can compile them using mkdocs serve , and use Python\u2019s built-in web server for a preview in your web browser ( http://localhost:8000 ). See the Extra docs packages section below for more details. Code Contributions There are several ways to contribute code to the project, but first, create a user account on GitHub if you do not already have one. Any contributor changes should be reported as an issue, or proposed as a pull request (PR) from their own fork . Submit an Issue Before you work on any non-trivial code contribution, it\u2019s best to first create a report in the issue tracker to start a discussion on the subject. This often provides additional considerations and avoids unnecessary work. Submit Your Contribution You can contribute code by cloning the repository content from your fork, then creating a new branch, setting up a local development environment , making changes, then submitting a pull request to review and accept those changes. Implement your changes on a new branch After cloning, create a branch to hold your changes then start making edits. Try to avoid working on the master/main branch if possible to avoid triggering automated build steps later on. use git to create a new branch. git checkout -b my-feat-branch Start your work on my-feat-branch branch. Don\u2019t forget to add docstrings to new functions, modules and classes, especially if they are part of public APIs. When you\u2019re done editing, and to record your changes in git , do: git add <MODIFIED FILES> git commit Format your commit messages so that they help create the appropriate semantic version number using conventional commit messages. Conventional Commit Resources : semantic-release : GitHub Action used with this package Other conventional commit examples and resources Commit History Writing a descriptive commit message is highly recommended. In case of doubt, you can check the commit history to look for recurring communication patterns with: git log --graph --decorate --pretty=oneline --abbrev-commit --all Submitting a pull request If everything works fine, push your local branch to your GitHub repository fork with: git push -u origin my-feat-branch Go to the web page of your fork and click \u201cCreate pull request\u201d to send your changes for review. Find more detailed information in creating a PR . You might also want to open the PR as a draft first and mark it as ready for review after getting feedback from the continuous integration (CI) system, or any required fixes. Setting up a Local Development Environment See the Installation section from the introductory documentation to first setup a local python virtual environment. Install the extra packages needed for local development, tests, and documentation: cd \"brain-lab\" conda activate brain_lab pip install -e \".[dev,doc,test,sciops]\" Develop Mode Installs The command conda list will show brain-lab , but it will be installed in editable/develop mode due to using the -e option during installation. This means that changes will be immediately reflected when the module is reloaded. Extra dev packages The list of dev packages are specified in pyproject.toml under [project.optional-dependencies.dev] nox The nox python package is used to automate a lot of the tasks, such as testing or building the documentation. For a list of command-line options, see: nox --help You can also use nox to run several other pre-configured tasks for this project. Try nox -l to see a list of the available tasks/sessions. pre-commit The pre-commit package is used to detect or fix common issues before code is submitted to the remote repository. You first have to set and install the hooks after cloning the repository from GitHub. The brain-lab package comes with a lot of hooks configured to automatically help you check the code after being written. View the configuration in the file .pre-commit-config.yaml . Installing pre-commit and the specificed hooks is required only once: pre-commit install --install-hooks You might also want to run pre-commit autoupdate to get the latest version of the hooks. You can also do a first pass of pre-commit to fix anything that needs fixing so that any future commits only process the necessary files: pre-commit run --all-files Tip The -n, --no-verify flag of git commit can be used to deactivate pre-commit hooks temporarily. Run pre-commit uninstall to permanently stop pre-commit. If you want to install pre-commit automatically for all newly cloned or initialized respositories that have a .pre-commit-config.yaml file, set the get config option and initialize the template, like so: git config --global init.templateDir ~/.git-template pre-commit init-templatedir ~/.git-template Extra test packages The list of test packages are specified in pyproject.toml under [project.optional-dependencies.test] pytest The pytest package is used to run integration and unit tests in the ./tests subdirectory. You can run pytest directly, see the help: pytest --help or use nox to run the pre-specified tests in the noxfile.py file. nox -s pytest Important Don\u2019t forget to add unit tests and documentation in case your contribution adds an additional feature and is not just a bugfix. Then please check that your changes don\u2019t break any unit tests with: nox -s pytest Extra docs packages The list of docs packages are specified in pyproject.toml under [project.optional-dependencies.doc] The first time you clone your repo, run the following command to set the default docs alias to latest and create the gh-pages branch on remote. This also creates an index.html file at the root of the pages branch. mike set-default --push latest Use nox to build and deploy the latest docs (swap out v0.0 for the latest version number): nox -s docs -- --version v0.0 You can also build and deploy the latest docs with the following command: mike deploy --push --update-aliases v0.0 latest mike serve See help for more information: mike deploy --help GitHub Pages Settings If your pages url is not loading, make sure you set the Source in your pages settings to gh-pages and use the root directory. Pages will then be built and deployed with the pages url as the default. Documentation Resources : builder: mkdocs builder: mike theme: mkdocs-material markdown extensions: python-markdown , pymdown-extensions plugin: mkdocstrings Environment maintenance Exporting python packages: mamba env export -n brain_lab -f environment.lock.yml --no-builds Resetting environment to locked state: mamba env update -f environment.lock.yml --prune Continuous Integration/Development The GitHub actions and workflows are located under the .github folder and automate many of the tasks previously outlined. The following actions are used: Checking out repo content: actions/checkout@v3 Caching python envs: actions/cache@v2 Version tagging from commits: mathieudutour/github-tag-action@v6.0 Create a release: ncipollo/release-action@v1 Using nox : excitedleigh/setup-nox@v2 Reset Tags and Releases GitHub actions will automatically create tags and releases on the main branch. To reset these, run: git tag -d $( git tag -l ) git fetch git push origin --delete $( git tag -l ) git tag -d $( git tag -l ) Integrated Development Environments Visual Studio Code The file brain-lab.code-workspace contains many of the vscode specific settings for working with this project. Of most importance are \"settings\": specific to this project and \"tasks\": to run common operations. extensions Stores a list of recommended extensions to install. To instal them, open the command pallete then type exten and select \u2018Extensions: Show Recommended Extensions\u2019 from the dropdown menu. folders Specify folders for multiroot workspaces. launch Specify python debugger configurations. settings Python specific settings as well as cookiecutter template settings. Be sure to specify the values of settings starting with djcookiecutter. . These are used for some of the vscode tasks. tasks Various tasks for environment setup, templates, or docker commands. Troubleshooting The following tips can be used when facing problems to build or test the package: Make sure to fetch all the tags from the upstream repository . The command git describe --abbrev=0 --tags should return the version you are expecting. If you are trying to run CI scripts in a forked repository, make sure to push all the tags. You can also try to remove all the egg files or the complete egg folder, i.e., .eggs , as well as the *.egg-info folders in the src folder or potentially in the root of your project. Pytest can drop you in an interactive session in the case an error occurs. In order to do that you need to pass a --pdb option (for example by running pytest <NAME OF THE FALLING TEST> --pdb ). You can also setup breakpoints manually instead of using the --pdb option.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-and-developer-guide","text":"Welcome to the brain-lab contributor\u2019s guide. This document focuses on getting any potential contributor familiarized with the DataJoint workflow development process, but other general kinds of contributions are also appreciated. See the DataJoint community contributions page for information. If you are new to using git or have never collaborated on a project previously, please have a look at contribution-guide.org . Other resources are also listed in the excellent guide created by FreeCodeCamp . Python Code of Conduct Please notice that all users and contributors are expected to be open, considerate, reasonable, and respectful . When in doubt, Python Software Foundation\u2019s Code of Conduct is a good reference in terms of behavior guidelines.","title":"Contributing and Developer Guide"},{"location":"CONTRIBUTING/#issue-reports","text":"If you experience bugs or general issues with brain-lab , please have a look at the issue tracker . If you don\u2019t see anything useful there, please feel free to fill out a new issue report. New issue reports should include information about your programming environment (e.g., operating system, Python version) and steps to reproduce the problem. Please also try to simplify the reproduction steps to a very minimal example that still illustrates the problem you\u2019re facing. By removing other factors, you help us to identify the root cause of the issue. Closed Issues Please don\u2019t forget to include the closed issues in your search. Sometimes a solution was already reported, and the problem is considered solved .","title":"Issue Reports"},{"location":"CONTRIBUTING/#documentation-improvements","text":"You can help improve the brain-lab docs by making them more readable and coherent, or by adding missing information and correcting mistakes. Quick Documentation Changes Please notice that the GitHub web interface provides a quick way of propose changes in brain-lab \u2018s files. While this mechanism can be tricky for normal code contributions, it works perfectly fine for contributing to the docs, and can be quite handy. brain-lab documentation uses MkDocs as its main documentation compiler. This means that the docs are kept in the same repository as the project code, and that any documentation update is done in the same way as a code contribution. All documentation is written in Markdown , specifically the python-markdown implementation with additional extensions to use features from reStructuredText. Google Style Docstrings Most documentation is attached directly to python objects and written in the Google style to faciliate markdown parsing. See Google style docstrings for examples. When working on documentation changes in your local machine, you can compile them using mkdocs serve , and use Python\u2019s built-in web server for a preview in your web browser ( http://localhost:8000 ). See the Extra docs packages section below for more details.","title":"Documentation Improvements"},{"location":"CONTRIBUTING/#code-contributions","text":"There are several ways to contribute code to the project, but first, create a user account on GitHub if you do not already have one. Any contributor changes should be reported as an issue, or proposed as a pull request (PR) from their own fork .","title":"Code Contributions"},{"location":"CONTRIBUTING/#submit-an-issue","text":"Before you work on any non-trivial code contribution, it\u2019s best to first create a report in the issue tracker to start a discussion on the subject. This often provides additional considerations and avoids unnecessary work.","title":"Submit an Issue"},{"location":"CONTRIBUTING/#submit-your-contribution","text":"You can contribute code by cloning the repository content from your fork, then creating a new branch, setting up a local development environment , making changes, then submitting a pull request to review and accept those changes.","title":"Submit Your Contribution"},{"location":"CONTRIBUTING/#implement-your-changes-on-a-new-branch","text":"After cloning, create a branch to hold your changes then start making edits. Try to avoid working on the master/main branch if possible to avoid triggering automated build steps later on. use git to create a new branch. git checkout -b my-feat-branch Start your work on my-feat-branch branch. Don\u2019t forget to add docstrings to new functions, modules and classes, especially if they are part of public APIs. When you\u2019re done editing, and to record your changes in git , do: git add <MODIFIED FILES> git commit Format your commit messages so that they help create the appropriate semantic version number using conventional commit messages. Conventional Commit Resources : semantic-release : GitHub Action used with this package Other conventional commit examples and resources Commit History Writing a descriptive commit message is highly recommended. In case of doubt, you can check the commit history to look for recurring communication patterns with: git log --graph --decorate --pretty=oneline --abbrev-commit --all","title":"Implement your changes on a new branch"},{"location":"CONTRIBUTING/#submitting-a-pull-request","text":"If everything works fine, push your local branch to your GitHub repository fork with: git push -u origin my-feat-branch Go to the web page of your fork and click \u201cCreate pull request\u201d to send your changes for review. Find more detailed information in creating a PR . You might also want to open the PR as a draft first and mark it as ready for review after getting feedback from the continuous integration (CI) system, or any required fixes.","title":"Submitting a pull request"},{"location":"CONTRIBUTING/#setting-up-a-local-development-environment","text":"See the Installation section from the introductory documentation to first setup a local python virtual environment. Install the extra packages needed for local development, tests, and documentation: cd \"brain-lab\" conda activate brain_lab pip install -e \".[dev,doc,test,sciops]\" Develop Mode Installs The command conda list will show brain-lab , but it will be installed in editable/develop mode due to using the -e option during installation. This means that changes will be immediately reflected when the module is reloaded.","title":"Setting up a Local Development Environment"},{"location":"CONTRIBUTING/#extra-dev-packages","text":"The list of dev packages are specified in pyproject.toml under [project.optional-dependencies.dev]","title":"Extra dev packages"},{"location":"CONTRIBUTING/#nox","text":"The nox python package is used to automate a lot of the tasks, such as testing or building the documentation. For a list of command-line options, see: nox --help You can also use nox to run several other pre-configured tasks for this project. Try nox -l to see a list of the available tasks/sessions.","title":"nox"},{"location":"CONTRIBUTING/#pre-commit","text":"The pre-commit package is used to detect or fix common issues before code is submitted to the remote repository. You first have to set and install the hooks after cloning the repository from GitHub. The brain-lab package comes with a lot of hooks configured to automatically help you check the code after being written. View the configuration in the file .pre-commit-config.yaml . Installing pre-commit and the specificed hooks is required only once: pre-commit install --install-hooks You might also want to run pre-commit autoupdate to get the latest version of the hooks. You can also do a first pass of pre-commit to fix anything that needs fixing so that any future commits only process the necessary files: pre-commit run --all-files Tip The -n, --no-verify flag of git commit can be used to deactivate pre-commit hooks temporarily. Run pre-commit uninstall to permanently stop pre-commit. If you want to install pre-commit automatically for all newly cloned or initialized respositories that have a .pre-commit-config.yaml file, set the get config option and initialize the template, like so: git config --global init.templateDir ~/.git-template pre-commit init-templatedir ~/.git-template","title":"pre-commit"},{"location":"CONTRIBUTING/#extra-test-packages","text":"The list of test packages are specified in pyproject.toml under [project.optional-dependencies.test]","title":"Extra test packages"},{"location":"CONTRIBUTING/#pytest","text":"The pytest package is used to run integration and unit tests in the ./tests subdirectory. You can run pytest directly, see the help: pytest --help or use nox to run the pre-specified tests in the noxfile.py file. nox -s pytest Important Don\u2019t forget to add unit tests and documentation in case your contribution adds an additional feature and is not just a bugfix. Then please check that your changes don\u2019t break any unit tests with: nox -s pytest","title":"pytest"},{"location":"CONTRIBUTING/#extra-docs-packages","text":"The list of docs packages are specified in pyproject.toml under [project.optional-dependencies.doc] The first time you clone your repo, run the following command to set the default docs alias to latest and create the gh-pages branch on remote. This also creates an index.html file at the root of the pages branch. mike set-default --push latest Use nox to build and deploy the latest docs (swap out v0.0 for the latest version number): nox -s docs -- --version v0.0 You can also build and deploy the latest docs with the following command: mike deploy --push --update-aliases v0.0 latest mike serve See help for more information: mike deploy --help GitHub Pages Settings If your pages url is not loading, make sure you set the Source in your pages settings to gh-pages and use the root directory. Pages will then be built and deployed with the pages url as the default. Documentation Resources : builder: mkdocs builder: mike theme: mkdocs-material markdown extensions: python-markdown , pymdown-extensions plugin: mkdocstrings","title":"Extra docs packages"},{"location":"CONTRIBUTING/#environment-maintenance","text":"Exporting python packages: mamba env export -n brain_lab -f environment.lock.yml --no-builds Resetting environment to locked state: mamba env update -f environment.lock.yml --prune","title":"Environment maintenance"},{"location":"CONTRIBUTING/#continuous-integrationdevelopment","text":"The GitHub actions and workflows are located under the .github folder and automate many of the tasks previously outlined. The following actions are used: Checking out repo content: actions/checkout@v3 Caching python envs: actions/cache@v2 Version tagging from commits: mathieudutour/github-tag-action@v6.0 Create a release: ncipollo/release-action@v1 Using nox : excitedleigh/setup-nox@v2 Reset Tags and Releases GitHub actions will automatically create tags and releases on the main branch. To reset these, run: git tag -d $( git tag -l ) git fetch git push origin --delete $( git tag -l ) git tag -d $( git tag -l )","title":"Continuous Integration/Development"},{"location":"CONTRIBUTING/#integrated-development-environments","text":"","title":"Integrated Development Environments"},{"location":"CONTRIBUTING/#visual-studio-code","text":"The file brain-lab.code-workspace contains many of the vscode specific settings for working with this project. Of most importance are \"settings\": specific to this project and \"tasks\": to run common operations. extensions Stores a list of recommended extensions to install. To instal them, open the command pallete then type exten and select \u2018Extensions: Show Recommended Extensions\u2019 from the dropdown menu. folders Specify folders for multiroot workspaces. launch Specify python debugger configurations. settings Python specific settings as well as cookiecutter template settings. Be sure to specify the values of settings starting with djcookiecutter. . These are used for some of the vscode tasks. tasks Various tasks for environment setup, templates, or docker commands.","title":"Visual Studio Code"},{"location":"CONTRIBUTING/#troubleshooting","text":"The following tips can be used when facing problems to build or test the package: Make sure to fetch all the tags from the upstream repository . The command git describe --abbrev=0 --tags should return the version you are expecting. If you are trying to run CI scripts in a forked repository, make sure to push all the tags. You can also try to remove all the egg files or the complete egg folder, i.e., .eggs , as well as the *.egg-info folders in the src folder or potentially in the root of your project. Pytest can drop you in an interactive session in the case an error occurs. In order to do that you need to pass a --pdb option (for example by running pytest <NAME OF THE FALLING TEST> --pdb ). You can also setup breakpoints manually instead of using the --pdb option.","title":"Troubleshooting"},{"location":"LICENSE/","text":"The MIT License (MIT) Copyright (c) 2022 DataJoint Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Open Source License"},{"location":"api/SUMMARY/","text":"brain_lab pipeline populate entrypoint support tables utils","title":"SUMMARY"},{"location":"api/brain_lab/","text":"brain_lab brain-lab : A Science Institute Workflow for Brain Lab get_version get_version () Get version number for the package brain-lab . RETURNS DESCRIPTION str Version number taken from the installed package version or version.py . TYPE: str command-line interface Module-level command-line interface This serves as an example command-line interface for the main package module. Example Usage as a package cli: brain_lab --help brain_lab --version Usage as a script: python -m brain_lab Usage from python: from brain_lab import main; main(...) parse_args parse_args ( args ) Parse command line parameters PARAMETER DESCRIPTION args Command line parameters as list of strings. (example [\"--help\"] ) TYPE: list [ str ] RETURNS DESCRIPTION argparse . Namespace A Namespace of command line parameters. main main ( ** kwargs ) Operate on cli args . cli cli () Calls __main__.main , passing the cli arguments extracted from sys.argv . This function can be used as entry point to create console scripts on package install.","title":"brain_lab"},{"location":"api/brain_lab/#brain_lab","text":"brain-lab : A Science Institute Workflow for Brain Lab","title":"brain_lab"},{"location":"api/brain_lab/#brain_lab.get_version","text":"get_version () Get version number for the package brain-lab . RETURNS DESCRIPTION str Version number taken from the installed package version or version.py . TYPE: str","title":"get_version()"},{"location":"api/brain_lab/#command-line-interface","text":"Module-level command-line interface This serves as an example command-line interface for the main package module. Example Usage as a package cli: brain_lab --help brain_lab --version Usage as a script: python -m brain_lab Usage from python: from brain_lab import main; main(...)","title":"command-line interface"},{"location":"api/brain_lab/#brain_lab.__main__.parse_args","text":"parse_args ( args ) Parse command line parameters PARAMETER DESCRIPTION args Command line parameters as list of strings. (example [\"--help\"] ) TYPE: list [ str ] RETURNS DESCRIPTION argparse . Namespace A Namespace of command line parameters.","title":"parse_args()"},{"location":"api/brain_lab/#brain_lab.__main__.main","text":"main ( ** kwargs ) Operate on cli args .","title":"main()"},{"location":"api/brain_lab/#brain_lab.__main__.cli","text":"cli () Calls __main__.main , passing the cli arguments extracted from sys.argv . This function can be used as entry point to create console scripts on package install.","title":"cli()"},{"location":"api/brain_lab/pipeline/","text":"","title":"pipeline"},{"location":"api/brain_lab/populate/","text":"brain_lab.populate","title":"populate"},{"location":"api/brain_lab/populate/#brain_lab.populate","text":"","title":"populate"},{"location":"api/brain_lab/populate/entrypoint/","text":"brain_lab.populate.entrypoint Entrypoint for ingestion routines This serves as an example command-line entrypoint for running different DataJoint populate functions for data ingestion. Requires a valid connection to a database. The script will run the run() function in this module and requires content from the brain_lab package. Example Usage as a console entrypoint: brain_lab_entrypoint --help brain_lab_entrypoint task1 brain_lab_entrypoint task2 -d 600 -s 60 brain_lab_entrypoint -vvv task1 Usage as a script: python populate/entrypoint.py --help Usage from python: from brain_lab_entrypoint import run run(task=..., duration=20, sleep=5) ATTRIBUTE DESCRIPTION LOGGER Module level logger when specifying verbosity. TYPE: logging . Logger parse_args parse_args ( args ) Parse command line parameters PARAMETER DESCRIPTION args Command line parameters as list of strings. (example [\"--help\"] ) TYPE: list [ str ] RETURNS DESCRIPTION argparse . Namespace A Namespace of command line parameters. Source code in brain_lab/populate/entrypoint.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def parse_args ( args : Sequence [ str ]) -> argparse . Namespace : \"\"\"_Parse command line parameters_ Args: args (list[str]): Command line parameters as list of strings. (example `[\"--help\"]`) Returns: A Namespace of command line parameters. \"\"\" class ArgumentDefaultsRawDescriptionHelpFormatter ( argparse . ArgumentDefaultsHelpFormatter , argparse . RawDescriptionHelpFormatter , argparse . MetavarTypeHelpFormatter , ): \"\"\"Combination of different formatters\"\"\" pass class MultiplyArg ( argparse . Action ): \"\"\"Custom action to multiply value of user arg\"\"\" def __init__ ( self , option_strings , multiplier = 1 , * args , ** kwargs ): self . mult = multiplier super ( MultiplyArg , self ) . __init__ ( option_strings = option_strings , * args , ** kwargs ) def __call__ ( self , parser , namespace , values , option_string = None ): setattr ( namespace , self . dest , self . mult * values if values > 0 else values ) parser = argparse . ArgumentParser ( description = __doc__ , formatter_class = ArgumentDefaultsRawDescriptionHelpFormatter ) parser . add_argument ( \"task\" , help = \"The type of task/job to execute.\" , type = str , choices = [ \"task1\" , \"task2\" , \"task3\" ], ) parser . add_argument ( \"-d\" , \"--duration\" , dest = \"run_duration\" , help = \"Specify the length of time for which to run the task (in hours).\" \"A negative duration will run in an infinite loop.\" , type = float , default =- 1 , action = MultiplyArg , multiplier = 3600 , ) parser . add_argument ( \"-s\" , \"--sleep\" , dest = \"sleep_duration\" , help = \"Time to sleep in between loops (in minutes)\" , type = float , default = 1 , action = MultiplyArg , multiplier = 60 , ) parser . add_argument ( \"-b\" , \"--backtrack\" , dest = \"backtrack_days\" , help = \"The number of days in the past for which to compare records.\" , type = int , default = 3 , ) parser . add_argument ( \"-x\" , \"--xtable\" , dest = \"exclude_tables\" , help = \"Exclude a behavior table. Can be used multiple times.\" , nargs = \"+\" , type = str , action = \"extend\" , ) parser . add_argument ( \"--xplots\" , dest = \"exclude_plots\" , help = \"Exclude populating any plot from a populate routine.\" , action = \"store_true\" , ) parser . add_argument ( \"--verbose\" , \"-v\" , dest = \"loglevel\" , help = \"Increase logging verbosity. Can be used up to 2 times \" \"at base level 'WARNING'.\" , action = \"count\" , default = 0 , ) parsed_args = parser . parse_args ( args ) return parsed_args setup_logging setup_logging ( loglevel , base_level = 'WARNING' ) Setup basic logging PARAMETER DESCRIPTION loglevel Minimum log level number for emitting messages. TYPE: int base_level Base log level name. TYPE: str DEFAULT: 'WARNING' Source code in brain_lab/populate/entrypoint.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def setup_logging ( loglevel : int , base_level : str = \"WARNING\" ) -> logging . Logger : \"\"\"_Setup basic logging_ Args: loglevel (int): Minimum log level number for emitting messages. base_level (str, optional): Base log level name. \"\"\" if loglevel is None or loglevel < 1 : loglevel = logging . getLevelName ( dj . config . get ( \"loglevel\" , base_level )) else : base_loglevel = logging . getLevelName ( base_level ) loglevel = base_loglevel - ( 10 * int ( min ( loglevel , base_loglevel / 10 ))) LOGGER . handlers = [] LOGGER . setLevel ( loglevel ) print ( f \"logger set to level: ' { logging . getLevelName ( loglevel ) } '\" ) std_out_handler = logging . StreamHandler ( stream = sys . stdout ) std_out_handler . setLevel ( loglevel ) formatter = logging . Formatter ( fmt = \"[ %(asctime)s %(process)d %(processName)s \" \" %(levelname)s %(name)s ]: %(message)s \" , datefmt = \"%z %Y-%m- %d %H:%M:%S\" , ) std_out_handler . setFormatter ( formatter ) LOGGER . addHandler ( std_out_handler ) return LOGGER run run ( ** kwargs ) Run ingestion routine depending on the configured task/job See Example for a list of args. Source code in brain_lab/populate/entrypoint.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def run ( ** kwargs : Any ) -> None : \"\"\"_Run ingestion routine depending on the configured task/job_ See [Example][brain_lab.populate.entrypoint] for a list of args. \"\"\" setup_logging ( kwargs . get ( \"loglevel\" , 0 )) LOGGER . info ( \"Starting ingestion process.\" ) try : LOGGER . debug ( f \"Running task: ' { kwargs [ 'task' ] } '\" ) # do something ... except Exception : LOGGER . exception ( f \"action ' { kwargs [ 'task' ] } ' encountered an exception:\" ) LOGGER . info ( \"Ingestion process ended.\" ) cli cli () Calls entrypoint.run , passing the cli arguments extracted from sys.argv . This function can be used as entry point to create console scripts on package install. Source code in brain_lab/populate/entrypoint.py 200 201 202 203 204 205 206 207 208 209 210 def cli () -> None : \"\"\"_Calls [`entrypoint.run`][brain_lab.populate.entrypoint.run], passing the cli arguments extracted from `sys.argv`_. This function can be used as entry point to create console scripts on package install. \"\"\" args = parse_args ( sys . argv [ 1 :]) run ( ** vars ( args ))","title":"entrypoint"},{"location":"api/brain_lab/populate/entrypoint/#brain_lab.populate.entrypoint","text":"Entrypoint for ingestion routines This serves as an example command-line entrypoint for running different DataJoint populate functions for data ingestion. Requires a valid connection to a database. The script will run the run() function in this module and requires content from the brain_lab package. Example Usage as a console entrypoint: brain_lab_entrypoint --help brain_lab_entrypoint task1 brain_lab_entrypoint task2 -d 600 -s 60 brain_lab_entrypoint -vvv task1 Usage as a script: python populate/entrypoint.py --help Usage from python: from brain_lab_entrypoint import run run(task=..., duration=20, sleep=5) ATTRIBUTE DESCRIPTION LOGGER Module level logger when specifying verbosity. TYPE: logging . Logger","title":"entrypoint"},{"location":"api/brain_lab/populate/entrypoint/#brain_lab.populate.entrypoint.parse_args","text":"parse_args ( args ) Parse command line parameters PARAMETER DESCRIPTION args Command line parameters as list of strings. (example [\"--help\"] ) TYPE: list [ str ] RETURNS DESCRIPTION argparse . Namespace A Namespace of command line parameters. Source code in brain_lab/populate/entrypoint.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def parse_args ( args : Sequence [ str ]) -> argparse . Namespace : \"\"\"_Parse command line parameters_ Args: args (list[str]): Command line parameters as list of strings. (example `[\"--help\"]`) Returns: A Namespace of command line parameters. \"\"\" class ArgumentDefaultsRawDescriptionHelpFormatter ( argparse . ArgumentDefaultsHelpFormatter , argparse . RawDescriptionHelpFormatter , argparse . MetavarTypeHelpFormatter , ): \"\"\"Combination of different formatters\"\"\" pass class MultiplyArg ( argparse . Action ): \"\"\"Custom action to multiply value of user arg\"\"\" def __init__ ( self , option_strings , multiplier = 1 , * args , ** kwargs ): self . mult = multiplier super ( MultiplyArg , self ) . __init__ ( option_strings = option_strings , * args , ** kwargs ) def __call__ ( self , parser , namespace , values , option_string = None ): setattr ( namespace , self . dest , self . mult * values if values > 0 else values ) parser = argparse . ArgumentParser ( description = __doc__ , formatter_class = ArgumentDefaultsRawDescriptionHelpFormatter ) parser . add_argument ( \"task\" , help = \"The type of task/job to execute.\" , type = str , choices = [ \"task1\" , \"task2\" , \"task3\" ], ) parser . add_argument ( \"-d\" , \"--duration\" , dest = \"run_duration\" , help = \"Specify the length of time for which to run the task (in hours).\" \"A negative duration will run in an infinite loop.\" , type = float , default =- 1 , action = MultiplyArg , multiplier = 3600 , ) parser . add_argument ( \"-s\" , \"--sleep\" , dest = \"sleep_duration\" , help = \"Time to sleep in between loops (in minutes)\" , type = float , default = 1 , action = MultiplyArg , multiplier = 60 , ) parser . add_argument ( \"-b\" , \"--backtrack\" , dest = \"backtrack_days\" , help = \"The number of days in the past for which to compare records.\" , type = int , default = 3 , ) parser . add_argument ( \"-x\" , \"--xtable\" , dest = \"exclude_tables\" , help = \"Exclude a behavior table. Can be used multiple times.\" , nargs = \"+\" , type = str , action = \"extend\" , ) parser . add_argument ( \"--xplots\" , dest = \"exclude_plots\" , help = \"Exclude populating any plot from a populate routine.\" , action = \"store_true\" , ) parser . add_argument ( \"--verbose\" , \"-v\" , dest = \"loglevel\" , help = \"Increase logging verbosity. Can be used up to 2 times \" \"at base level 'WARNING'.\" , action = \"count\" , default = 0 , ) parsed_args = parser . parse_args ( args ) return parsed_args","title":"parse_args()"},{"location":"api/brain_lab/populate/entrypoint/#brain_lab.populate.entrypoint.setup_logging","text":"setup_logging ( loglevel , base_level = 'WARNING' ) Setup basic logging PARAMETER DESCRIPTION loglevel Minimum log level number for emitting messages. TYPE: int base_level Base log level name. TYPE: str DEFAULT: 'WARNING' Source code in brain_lab/populate/entrypoint.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def setup_logging ( loglevel : int , base_level : str = \"WARNING\" ) -> logging . Logger : \"\"\"_Setup basic logging_ Args: loglevel (int): Minimum log level number for emitting messages. base_level (str, optional): Base log level name. \"\"\" if loglevel is None or loglevel < 1 : loglevel = logging . getLevelName ( dj . config . get ( \"loglevel\" , base_level )) else : base_loglevel = logging . getLevelName ( base_level ) loglevel = base_loglevel - ( 10 * int ( min ( loglevel , base_loglevel / 10 ))) LOGGER . handlers = [] LOGGER . setLevel ( loglevel ) print ( f \"logger set to level: ' { logging . getLevelName ( loglevel ) } '\" ) std_out_handler = logging . StreamHandler ( stream = sys . stdout ) std_out_handler . setLevel ( loglevel ) formatter = logging . Formatter ( fmt = \"[ %(asctime)s %(process)d %(processName)s \" \" %(levelname)s %(name)s ]: %(message)s \" , datefmt = \"%z %Y-%m- %d %H:%M:%S\" , ) std_out_handler . setFormatter ( formatter ) LOGGER . addHandler ( std_out_handler ) return LOGGER","title":"setup_logging()"},{"location":"api/brain_lab/populate/entrypoint/#brain_lab.populate.entrypoint.run","text":"run ( ** kwargs ) Run ingestion routine depending on the configured task/job See Example for a list of args. Source code in brain_lab/populate/entrypoint.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def run ( ** kwargs : Any ) -> None : \"\"\"_Run ingestion routine depending on the configured task/job_ See [Example][brain_lab.populate.entrypoint] for a list of args. \"\"\" setup_logging ( kwargs . get ( \"loglevel\" , 0 )) LOGGER . info ( \"Starting ingestion process.\" ) try : LOGGER . debug ( f \"Running task: ' { kwargs [ 'task' ] } '\" ) # do something ... except Exception : LOGGER . exception ( f \"action ' { kwargs [ 'task' ] } ' encountered an exception:\" ) LOGGER . info ( \"Ingestion process ended.\" )","title":"run()"},{"location":"api/brain_lab/populate/entrypoint/#brain_lab.populate.entrypoint.cli","text":"cli () Calls entrypoint.run , passing the cli arguments extracted from sys.argv . This function can be used as entry point to create console scripts on package install. Source code in brain_lab/populate/entrypoint.py 200 201 202 203 204 205 206 207 208 209 210 def cli () -> None : \"\"\"_Calls [`entrypoint.run`][brain_lab.populate.entrypoint.run], passing the cli arguments extracted from `sys.argv`_. This function can be used as entry point to create console scripts on package install. \"\"\" args = parse_args ( sys . argv [ 1 :]) run ( ** vars ( args ))","title":"cli()"},{"location":"api/brain_lab/support/","text":"","title":"support"},{"location":"api/brain_lab/tables/","text":"","title":"tables"},{"location":"api/brain_lab/utils/","text":"","title":"utils"}]}